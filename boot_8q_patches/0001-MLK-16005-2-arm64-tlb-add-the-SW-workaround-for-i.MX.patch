From d45519d63f5026176bdd6fd4cfa1b46b0a057170 Mon Sep 17 00:00:00 2001
From: Li Yang <leoyang.li@nxp.com>
Date: Mon, 14 Aug 2023 15:22:21 -0500
Subject: [PATCH 1/6] MLK-16005-2 arm64: tlb: add the SW workaround for i.MX8QM
 TKT340553
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

on i.MX8QM 1.0/1.1,TLB maintenance through DVM messages over ARADDR channel,
some bits (see the following) will be corrupted:

ASID[15:12] VA[48:45] VA[44:41] VA[39:36]

This issue will result in the TLB aintenance across the clusters not working
as expected due to some VA and ASID bits get corrupted

The SW workaround is: use the vmalle1is if VA larger than 36bits or
ASID[15:12] is not zero, otherwise, we use original TLB maintenance path.

Note: To simplify the code, we did not check VA[40] bit specifically

As per latest i.MX8QM SOC Errata, TKT340553 workaround needs to be
updated to unconditionally downgrade TLB operations and instruction
cache maintenance.

Current workaround is looping uselessly on the address
range doing a  _tlbi(vmalle1is) which is harmful for
the system performance and buggy as the instruction is
flushing the entire TLB and there is no benefit of
redoing it more than once. Also fix missing barriers.

SinceÂ commit 967747bbc08 ("uaccess: remove CONFIG_SET_FS"), API
user_addr_max() has been removed.  Change it to TASK_SIZE_MAX as done
with other kernel code in upstream.

Since commit bacac637025 ("arm64: extable: cleanup redundant extable type EX_TYPE_FIXUP")
_ASM_EXTABLE is removed.  Change to use new API.

[ Leo: split arch change from drivers/soc change, squashed all updates ]
Change-Id: Ib514e68bf06e96e1eadc09afaa5f3cb1528ba4da
Signed-off-by: Jason Liu <jason.hui.liu@nxp.com>
Signed-off-by: Nitin Garg <nitin.garg@nxp.com>
Signed-off-by: Anson Huang <Anson.Huang@nxp.com>
Acked-by: Peng Fan <peng.fan@nxp.com>
Signed-off-by: Marouen Ghodhbane <marouen.ghodhbane@nxp.com>
Signed-off-by: Li Yang <leoyang.li@nxp.com>
---
 arch/arm64/include/asm/tlbflush.h | 43 ++++++++++++++++++++++++++-----
 arch/arm64/kernel/cpufeature.c    | 14 ++++++++++
 arch/arm64/kernel/traps.c         | 23 ++++++++++++++++-
 3 files changed, 72 insertions(+), 8 deletions(-)

diff --git a/arch/arm64/include/asm/tlbflush.h b/arch/arm64/include/asm/tlbflush.h
index f706c6fbada9..034b0faebbe4 100644
--- a/arch/arm64/include/asm/tlbflush.h
+++ b/arch/arm64/include/asm/tlbflush.h
@@ -17,6 +17,8 @@
 #include <asm/cputype.h>
 #include <asm/mmu.h>
 
+extern bool TKT340553_SW_WORKAROUND;
+
 /*
  * Raw TLBI operations.
  *
@@ -256,9 +258,16 @@ static inline void flush_tlb_mm(struct mm_struct *mm)
 
 	dsb(ishst);
 	asid = __TLBI_VADDR(0, ASID(mm));
-	__tlbi(aside1is, asid);
-	__tlbi_user(aside1is, asid);
-	dsb(ish);
+	if (TKT340553_SW_WORKAROUND) {
+		/* Flush the entire TLB */
+		__tlbi(vmalle1is);
+		dsb(ish);
+		isb();
+	} else {
+		__tlbi(aside1is, asid);
+		__tlbi_user(aside1is, asid);
+		dsb(ish);
+	}
 	mmu_notifier_arch_invalidate_secondary_tlbs(mm, 0, -1UL);
 }
 
@@ -269,8 +278,15 @@ static inline void __flush_tlb_page_nosync(struct mm_struct *mm,
 
 	dsb(ishst);
 	addr = __TLBI_VADDR(uaddr, ASID(mm));
-	__tlbi(vale1is, addr);
-	__tlbi_user(vale1is, addr);
+	if (TKT340553_SW_WORKAROUND) {
+		/* Flush the entire TLB */
+		__tlbi(vmalle1is);
+		dsb(ish);
+		isb();
+	} else {
+		__tlbi(vale1is, addr);
+		__tlbi_user(vale1is, addr);
+	}
 	mmu_notifier_arch_invalidate_secondary_tlbs(mm, uaddr & PAGE_MASK,
 						(uaddr & PAGE_MASK) + PAGE_SIZE);
 }
@@ -429,6 +445,14 @@ static inline void __flush_tlb_range_nosync(struct vm_area_struct *vma,
 	dsb(ishst);
 	asid = ASID(vma->vm_mm);
 
+	if (TKT340553_SW_WORKAROUND) {
+		/* Flush the entire TLB and exit */
+		__tlbi(vmalle1is);
+		dsb(ish);
+		isb();
+		return;
+	}
+
 	if (last_level)
 		__flush_tlb_range_op(vale1is, start, pages, stride, asid, tlb_level, true);
 	else
@@ -462,7 +486,8 @@ static inline void flush_tlb_kernel_range(unsigned long start, unsigned long end
 {
 	unsigned long addr;
 
-	if ((end - start) > (MAX_TLBI_OPS * PAGE_SIZE)) {
+	if (((end - start) > (MAX_TLBI_OPS * PAGE_SIZE))
+	    || (TKT340553_SW_WORKAROUND)) {
 		flush_tlb_all();
 		return;
 	}
@@ -486,7 +511,11 @@ static inline void __flush_tlb_kernel_pgtable(unsigned long kaddr)
 	unsigned long addr = __TLBI_VADDR(kaddr, 0);
 
 	dsb(ishst);
-	__tlbi(vaae1is, addr);
+	if (TKT340553_SW_WORKAROUND)
+		/* Flush the entire TLB */
+		__tlbi(vmalle1is);
+	else
+		__tlbi(vaae1is, addr);
 	dsb(ish);
 	isb();
 }
diff --git a/arch/arm64/kernel/cpufeature.c b/arch/arm64/kernel/cpufeature.c
index a19dd1a54f1a..c4fc43be913c 100644
--- a/arch/arm64/kernel/cpufeature.c
+++ b/arch/arm64/kernel/cpufeature.c
@@ -980,9 +980,23 @@ init_cpucap_indirect_list_from_array(const struct arm64_cpu_capabilities *caps)
 	}
 }
 
+bool TKT340553_SW_WORKAROUND;
+EXPORT_SYMBOL_GPL(TKT340553_SW_WORKAROUND);
 static void __init init_cpucap_indirect_list(void)
 {
 	init_cpucap_indirect_list_from_array(arm64_features);
+#ifdef CONFIG_ARM64_WORKAROUND_CLEAN_CACHE
+#if	defined(CONFIG_ARM64_ERRATUM_826319) || \
+	defined(CONFIG_ARM64_ERRATUM_827319) || \
+	defined(CONFIG_ARM64_ERRATUM_824069)
+	if (TKT340553_SW_WORKAROUND) {
+		struct midr_range *midr_range_list =
+			(struct midr_range *)(arm64_errata[0].midr_range_list);
+
+		midr_range_list[0].rv_max = MIDR_CPU_VAR_REV(0, 4);
+	}
+#endif
+#endif
 	init_cpucap_indirect_list_from_array(arm64_errata);
 }
 
diff --git a/arch/arm64/kernel/traps.c b/arch/arm64/kernel/traps.c
index c66a48d732c9..4a639b046281 100644
--- a/arch/arm64/kernel/traps.c
+++ b/arch/arm64/kernel/traps.c
@@ -592,6 +592,24 @@ void do_el0_mops(struct pt_regs *regs, unsigned long esr)
 		uaccess_ttbr0_disable();			\
 	}
 
+#define __user_cache_maint_ivau(insn, address, res)			\
+	do {								\
+		if (address >= TASK_SIZE_MAX) {			\
+			res = -EFAULT;					\
+		} else {						\
+			uaccess_ttbr0_enable();				\
+			asm volatile (					\
+				"1:	" insn "\n"			\
+				"	mov	%w0, #0\n"		\
+				"2:\n"					\
+				_ASM_EXTABLE_UACCESS_ERR(1b, 2b, %w0)	\
+				: "=r" (res)				\
+				: "r" (address));			\
+			uaccess_ttbr0_disable();			\
+		}							\
+	} while (0)
+
+extern bool TKT340553_SW_WORKAROUND;
 static void user_cache_maint_handler(unsigned long esr, struct pt_regs *regs)
 {
 	unsigned long tagged_address, address;
@@ -619,7 +637,10 @@ static void user_cache_maint_handler(unsigned long esr, struct pt_regs *regs)
 		__user_cache_maint("dc civac", address, ret);
 		break;
 	case ESR_ELx_SYS64_ISS_CRM_IC_IVAU:	/* IC IVAU */
-		__user_cache_maint("ic ivau", address, ret);
+		if (TKT340553_SW_WORKAROUND)
+			__user_cache_maint_ivau("ic ialluis", address, ret);
+		else
+			__user_cache_maint("ic ivau", address, ret);
 		break;
 	default:
 		force_signal_inject(SIGILL, ILL_ILLOPC, regs->pc, 0);
-- 
2.34.1

